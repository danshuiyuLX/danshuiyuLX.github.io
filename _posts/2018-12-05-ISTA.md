---  

title: ISTA算法  
date: 2018-12-05  
categories: Algorithm  
tags: Optimization Algorithm Matlab  

---

从无约束的优化问题开始：

$$
\min_x\{F(x)\equiv f(x)\} \tag1
$$  

假设$f(x)$满足Lipschitz连续条件：$f(x)$的导数有上界，那么其最小上界称为Lipschitz常数$L(f)$，这时，对于任意的$L \geq L(f)$，有：  

$$
f(x) \leq f(y) + \langle x-y, \triangledown f(y)\rangle + \frac{L}{2}\|x - y\|^2 \\ for \ every \ \ x, y \ \in \mathbb{R}^n \tag2
$$  

基于此，在点$x_{k- 1}$附近可以把函数值$f(x)$近似为：  

$$
\hat{f}(x, x_{k-1}) \doteq f(x_k)+\langle \triangledown f(x) , x - x_{k-1} \rangle + \frac{L}{2}\|x - x_{k - 1}\|^2 \tag3
$$  

这里$\hat{f}(x, x_{k - 1})$是一个函数，当$x_{k-1}$固定时，是关于$x$的函数。  


**此时$f(x)$的近似值$\hat{f}(x,x_{k-1})$是大于真实值的，因此如果找到点$x_{k - 1}$附近的最小近似值点，且该值比$f(x_{k-1})$小的话，那么可以令该最小近似值点为下次迭代的起始点$x_{k}$，此时真实值$f(x_k)<\hat{f}(x_k,x_{k-1})<f(x_{k-1})$**  

因此在梯度下降的每次迭代中，将点$x_{k-1}$处的近似函数取得最小值的点作为下一次迭代的起始点$x_k$，这就是所谓的proximal regularzation算法。  

$$
x_k = \arg \min_x\{f(x_{k-1}) + \langle x - x_{k - 1}, \triangledown f(x_{k - 1})\rangle\ + \\ \frac{1}{2t_k}\|x - x_{k - 1}\|^2\} \tag4
$$  

引入范数规范化函数$g(x)$对参数$x$进行约束：  

$$
\min\{F(x) \equiv f(x) + g(x) : x\in \mathbb{R}^n\} \tag5
$$  

那么在点$x_{k-1}$, $F(x)$的近似函数为：  

$$
Q_L(x, x_{k-1})  := f(x_{k-1}) + \langle x - x_{k-1}, \triangledown f(x_{k-1})\rangle + \frac{L}{2}\|x - x_{k-1}\|^ 2 + g(x) \tag6
$$  

这个函数有两个变量$x, x_{k-1}$和参数$L$。**令该函数取得最小值**的$x$表示为$p_L$：  

$$
p_L(x_{k-1}) := \arg \min_x{Q_L{}(x, x_{k-1}):x\in\mathbb{R}^n} \tag7
$$  

结合式$(6)$和式$(7)$，同时忽略一些常数项，$p_L(y)$可以写成：  

$$
p_L(x_{k-1}) = \arg\min_x\{g(x) + \frac{L}{2}\|x - (x_{k-1} - \frac{1}{L}\triangledown f(x_{k-1}))\|^2\} \tag8
$$  

换句话说当$x_{k-1}$和$L$确定后，$p_L(x_{k-1})$也是一个确定值，它表示在点$y$附近能令F(x)近似函数取值最小的点$x$。从式$(2)$中，我们可以知道$F(x)$的近似函数是大于$F(x)$的，所以$F(p_L(x_{k - 1})) < Q_L(p_L(x_{k - 1}),x_{k-1})$。  

那么使用ISTA解决带约束的优化问题时的基本迭代步骤为：  

$$
x_k = p_L(x_{k - 1}) = \arg\min_x\{g(x) + \frac{L}{2}\|x - (x_{k-1} - \frac{1}{L}\triangledown f(x_{k-1}))\|^2\} 
$$  

**当$g(x)$不存在时， 式$(8)$变为：**  

$$
x_k = \arg \min_x \{ \frac{L}{2}\|x - (x_{k-1} - \frac{1}{L} \triangledown f(x_{k-1}))\|^2\} \tag{9}
$$  

可以看到  

$$
x_k = x_{k - 1} - \frac{1}{L}\triangledown f(x_{k-1})
$$  

这相当于梯度下降法中的学习率变为了$\frac{1}{L}$  

固定步长的缺点是：Lipschitz常数不一定可知或者可计算。因此，可以使用带backtracking的ISTA  

#### ISTA求解LASSO问题以及Soft Thresholding  

ISTA算法适用问题的形式：

* 目标方程是$f+g$的形式
* $f$和$g$是凸的，$f$可导，$g$无所谓
* $g$要足够简单

而lasso问题的形式为：  

$$
\min. \frac{1}{2}\|y - Ax\|_2^2 + \lambda\|x\|_1
$$  

可以看到，$\|y - Ax\|_2^2$是可导且凸的，$\|x\|_1$则不可导，因此可以用ISTA算法求解。  

根据式$(9)$，可以得到：  

$$
x_k = \arg \min_x\{ \frac{1}{2t_k}\|x - (x_{k - 1} - t_k \triangledown f(x_{k-1})\|_2^2 +\lambda\|x\|_1\} \tag{10}
$$  

式$(10)​$同样是个优化问题。当$g(x)=\lambda\|x\|_1​$时，对其求导可以得到：  

$$
x_k = \tau_{\lambda t_k}(x_{k-1}-t_k \triangledown f(x_k - 1))
$$  

其中$\tau_{\alpha}(x)_i = sign(x_i)(\|x_i\| - \alpha)$为shrinkage operator   
 
ISTA算法求解lasso问题matlab程序参考如下    

```
% 主函数部分

A = rand(10, 100);
b = rand(10, 1);
x = zeros(100, 1);
lambda = 0.1;
L = max(eig(A' * A));

[f, gradient_f] = f_and_g(A, b);
x = ISTA(f, gradient_f, L, x, lambda);

 
function x = ISTA(f, gradient_f, L, x, lambda)

% F(x, lambda) = f(x) + lambda|x|_1
% f为目标函数可导部分的函数句柄
% gradient_f为f导数函数句柄
% L为Lipschitz constant

	tol = 1e-8;
	max_iter = 1e4;
	
	k = 1;
	while(k < max_iter)
	
		% update x
		gradient_f_x = gradient_f(x);
		x_temp = wthresh(x - (1 / L) * gradient_f_x, 's', ...
				lambda / L);
		    
		F_x = f(x) + lambda * sum(abs(x));
		F_temp = f(x_temp) + lambda * sum(abs(x));
		    
		% stop condition
		if abs(F_x - F_temp) < tol
		break
		end
		x = x_temp;
		k = k + 1;
		
	end
end

function [f, gradient_f] = f_and_g(A, b)

% f(x) = (1/2)||Ax - b||_2^2
% gradient_f = ATAx -ATb

		f = @f_x;
		function output = f_x(x)
		    output = (1 / 2) * norm(A * x - b);
		end
		gradient_f = @g_f_x;
		function output = g_f_x(x)
			output = A' * A *x - A' * b;
		end
end  
```

