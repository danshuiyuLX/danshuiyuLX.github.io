---  
title: EM算法  
date: 2018-12-07  
catagories: algorithm  
tags: algorithm optimization    

---

**用于最大似然估计隐藏参数**

给定训练样本$\{x^{(1)},x^{(2)},…,x^{(n)}\}$，样本之间相互独立。首先根据最大似然估计训练样本分布的参数：  

$$
\begin{equation}
\begin{aligned}
l(\theta)&=\sum_{i=1}^{n}\log p(x^{(i)};\theta)\\
&=\sum_{i=1}^{n}\log \sum_z p(x^{(i)},z^{(i)};\theta)
\end{aligned}
\end{equation}
$$  

这里可以看到，由于存在着隐藏变量$z$，上式$\log$中出现了加和，无法通过求导求极值。 

EM算法可以通过不断确定该似然函数的下界，得到令似然函数最大的参数$\theta$。  

<u>令$Q_i(z)$表示对于样本$i$ ，隐含变量$z$的某种分布</u>，那么上式可以写成：  

$$
    l(\theta) =\sum_{i=1}^n \log \sum_{z^{(i)}} Q_i(z^{(i)}) \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$  

已知Jensen不等式（来自于最优化），如果$f(x)$为concave function，那么：  

$$
f(E(x))>E(f(x))
$$  

因为对数函数是concave function，所以将$\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$看做是Jensen不等式中的随机变量$x$时，$\log \sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z)}$同于期望的函数，而对应的函数的期望为：  

$$
\sum_{z^{(i)}} Q_i(z) \log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$  

这样根据Jensen不等式：  

$$
\sum_{i = 1}^{n}\log \sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} \ge
\sum_{i=1}^{n}\sum_{z^{(i)}} Q_i(z^{(i)}) \log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} \\
l(\theta) \ge \sum_{i=1}^{n}\sum_{z^{(i)}} Q_i(z^{(i)}) \log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$  

也就是说，如果可以通过迭代的方式，交替更新$Q_i(z^{(i)})$和$\theta$使其最大化等式的右边，就可以得到最大化的$l(\theta)$。  


E-step：  

假设我们已知第$M$次迭代的结果$\theta^M$，那么  

$$
Q_i^{M+1}(z^{(i)}) = \arg\max \sum_z Q_i(z^{(i)})\log\frac{p(x^{(i)},z^{(i)};\theta^M)}{Q_i(z^{(i)})}
$$  

注意，log函数是strictly concave的，所以只有当$\log \frac{p(x^{(i)},z^{(i);\theta})}{Q_i(z^{(i)})}, i = 1...n$的值都相同时，能够取得最大值  

$$
\frac{p(x^{(i)},z^{(i)};\theta^M)}{Q_i^{M+1}(z^{(i)})} = c
$$  

同时因为$\sum_{z^{(i)}} Q_i(z^{(i)})=1$，那么：  

$$
\sum_{z^{(i)}}{p(x^{(i)},z^{(i)};\theta^M)} = c \sum_{z^{(i)}}{Q_i^{M+1}(z^{(i)})} = c\\
p(x^{(i)};\theta) = c\\
Q_i^{M+1}(z^{(i)}) = \frac{p(x^{(i)},z^{(i)};\theta^M)}{p(x^{(i)};\theta^M)}=p(z^{(i)}|x^{(i)};\theta^M)
$$  

也就是说$Q_i^{M+1}(z^{(i)})$为已知$\theta ^ M$和$x^{(i)}$的条件概率。  

M-step:  

得到了$Q_i^{M +1}(z^{(i)})$后，最大化$\theta$  

$$
\begin{aligned}
\theta^{M + 1} & = \arg \max_{\theta}\sum_{i = 1}^n\sum_{z^{(i)}}p(z^{(i)}|x^{(i)};\theta^M)\log \frac{p(x^{(i)},z^{(i)};\theta)}{p(z^{(i)}|x^{(i)};\theta^M)}\\
& = \arg\max_{\theta}\sum_{i = 1}^n\sum_{z^{(i)}}p(z^{(i)}|x^{(i)};\theta^M)\log p(x^{(i)},z^{(i)};\theta) \\
& = \arg\max_\theta \sum_{i = 1}^n \mathrm{E}_{z|x, \theta^{M}}(\log p(x^{(i)},z^{(i)}|\theta))
\end{aligned}
$$  

所以E步就是求得$(x,z)$的对数似然函数在已知$x$和$\theta^M$条件下的条件期望，而M步就是最大化$\theta$。  
