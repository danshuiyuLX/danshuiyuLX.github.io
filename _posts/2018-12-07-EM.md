---
title: EM算法

date: 2018-12-07

catagories: algorithm

tags: algorithm optimization

---

在学习高斯混合模型用于图像分割时，接触到了EM算法。当模型中存在着隐藏变量时，可以用EM算法迭代估计模型参数。[JerryLead](http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html)，和[zouxy09](https://blog.csdn.net/zouxy09/article/details/8537620)的博文详细的介绍了EM算法。我在他们文章的基础上，参考了一篇Utah大学Sean Borman的[EM算法讲义](https://www.cs.utah.edu/~piyush/teaching/EM_algorithm.pdf)，将EM算法思想简单整理如下：

给定训练样本$\{x^{(1)},x^{(2)},…,x^{(n)}\}$，样本之间相互独立。首先根据最大似然估计训练样本分布的参数：  

$$
\begin{equation}
\begin{aligned}
l(\theta)&=\sum_{i=1}^{n}\log p(x^{(i)};\theta)\\
&=\sum_{i=1}^{n}\log \sum_z p(x^{(i)},z^{(i)};\theta)
\end{aligned}
\end{equation}
$$

这里可以看到，由于存在着隐藏变量$z$，上式对数函数$\log$中出现了加和，无法通过求导求极值。 

EM算法可以通过不断确定该似然函数$l(\theta)$的下界，得到令似然函数最大的参数$\theta$。  

<u>假设$Q_i(z)$表示对于样本$i$ ，隐含变量$z$的某种分布</u>，那么上式可以写成：  
$$
l(\theta) =\sum_{i=1}^n \log \sum_{z^{(i)}} Q_i(z^{(i)}) \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$

已知Jensen不等式（来自于convex optimization），如果$f(x)$为concave function，那么：  

$$
f(E(x))>E(f(x))
$$

因为对数函数是concave function，所以将$\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$看做是Jensen不等式中的随机变量$x$时，$\log \sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z)}$同于期望的函数，而对应的函数的期望为：  

$$
\sum_{z^{(i)}} Q_i(z) \log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$

这样根据Jensen不等式：  

$$
\sum_{i = 1}^{n}\log \sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} \ge
\sum_{i=1}^{n}\sum_{z^{(i)}} Q_i(z^{(i)}) \log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} \\
l(\theta) \ge \sum_{i=1}^{n}\sum_{z^{(i)}} Q_i(z^{(i)}) \log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$

$$
\sum_{i=1}^{n}\sum_{z^{(i)}} Q_i(z^{(i)}) \log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} \tag{1}
$$



也就是说，式$(1)$建立了一个$l(\theta)$的下界。如果可以通过迭代的方式，交替更新$Q_i(z^{(i)})$和$\theta$使式$(1)$最大化，就可以得到最大化的$l(\theta)$。  


E-step：  

假设我们已知第$M$次迭代的结果$\theta^M$，那么  

$$
Q_i^{M+1}(z^{(i)}) = \arg\max \sum_z Q_i(z^{(i)})\log\frac{p(x^{(i)},z^{(i)};\theta^M)}{Q_i(z^{(i)})}
$$

注意，log函数是strictly concave的，所以只有当$\log \frac{p(x^{(i)},z^{(i);\theta})}{Q_i(z^{(i)})}, i = 1...n$的值都相同时，能够取得最大值  

$$
\frac{p(x^{(i)},z^{(i)};\theta^M)}{Q_i^{M+1}(z^{(i)})} = c
$$

同时因为$\sum_{z^{(i)}} Q_i(z^{(i)})=1$，那么：  

$$
\sum_{z^{(i)}}{p(x^{(i)},z^{(i)};\theta^M)} = c \sum_{z^{(i)}}{Q_i^{M+1}(z^{(i)})} = c\\
p(x^{(i)};\theta) = c\\
Q_i^{M+1}(z^{(i)}) = \frac{p(x^{(i)},z^{(i)};\theta^M)}{p(x^{(i)};\theta^M)}=p(z^{(i)}|x^{(i)};\theta^M)
$$

也就是说$Q_i^{M+1}(z^{(i)})$为已知$\theta ^ M$和$x^{(i)}$的条件概率。  

M-step:  

得到了$Q_i^{M +1}(z^{(i)})$后，最大化$\theta$  

$$
\begin{aligned}
\theta^{M + 1} & = \arg \max_{\theta}\sum_{i = 1}^n\sum_{z^{(i)}}p(z^{(i)}|x^{(i)};\theta^M)\log \frac{p(x^{(i)},z^{(i)};\theta)}{p(z^{(i)}|x^{(i)};\theta^M)}\\
& = \arg\max_{\theta}\sum_{i = 1}^n\sum_{z^{(i)}}p(z^{(i)}|x^{(i)};\theta^M)\log p(x^{(i)},z^{(i)};\theta) \\
& = \arg\max_\theta \sum_{i = 1}^n \mathrm{E}_{z|x, \theta^{M}}(\log p(x^{(i)},z^{(i)}|\theta))
\end{aligned}
$$

所以E步就是求得$(x,z)$的对数似然函数在已知$x$和$\theta^M$条件下的条件期望，而M步就是最大化$\theta$。  
